{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81ac873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_inline import backend_inline\n",
    "from IPython import display\n",
    "# Define the number of inputs, hidden units, and outputs\n",
    "\n",
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "\n",
    "# Initialize weights and biases using PyTorch\n",
    "# W1 = torch.randn(num_inputs, num_hiddens) * 0.01  # Normal distribution with mean=0, std=0.01\n",
    "# b1 = torch.zeros(num_hiddens)\n",
    "\n",
    "# W2 = torch.randn(num_hiddens, num_outputs) * 0.0`1  # Same for W2\n",
    "# b2 = torch.zeros(num_outputs)\n",
    "\n",
    "# batch_size = 256\n",
    "# num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "lr = 0.01\n",
    "\n",
    "# params = [W1, b1, W2, b2]\n",
    "\n",
    "# for param in params:\n",
    "#     param.requires_grad_()\n",
    "\n",
    "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "    \"\"\"设置matplotlib的轴\"\"\"\n",
    "    axes.set_xlabel(xlabel)\n",
    "    axes.set_ylabel(ylabel)\n",
    "    axes.set_xscale(xscale)\n",
    "    axes.set_yscale(yscale)\n",
    "    axes.set_xlim(xlim)\n",
    "    axes.set_ylim(ylim)\n",
    "    if legend:\n",
    "        axes.legend(legend)\n",
    "    axes.grid()\n",
    "    \n",
    "# def load_data_fashion_mnist(batch_size, resize=None):  #@save\n",
    "#     dataset = gluon.data.vision\n",
    "#     trans = [dataset.transforms.ToTensor()]\n",
    "#     if resize:\n",
    "#         trans.insert(0, dataset.transforms.Resize(resize))\n",
    "#     trans = dataset.transforms.Compose(trans)\n",
    "#     mnist_train = dataset.FashionMNIST(train=True).transform_first(trans)\n",
    "#     mnist_test = dataset.FashionMNIST(train=False).transform_first(trans)\n",
    "#     return (gluon.data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "#                                   num_workers=get_dataloader_workers()),\n",
    "#             gluon.data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
    "#                                   num_workers=get_dataloader_workers()))\n",
    "\n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None):\n",
    "     # 定义数据预处理（转换）\n",
    "    transform_list = [transforms.ToTensor()]\n",
    "    \n",
    "    if resize:\n",
    "        transform_list.insert(0, transforms.Resize(resize))\n",
    "    \n",
    "    transform = transforms.Compose(transform_list)\n",
    "\n",
    "    # 下载并加载训练集和测试集\n",
    "    train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    # 创建 DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=get_dataloader_workers())\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=get_dataloader_workers())\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def get_dataloader_workers():  #@save\n",
    "    return 10\n",
    "\n",
    "# train_iter, test_iter = load_data_fashion_mnist(batch_size)\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    _, preds = torch.max(y_hat, 1)  # Get predicted class indices\n",
    "    return (preds == y).sum().item()  # Return the correct number of predictions as a scalar (float)\n",
    "\n",
    "# #@save\n",
    "# class Accumulator: \n",
    "#     \"\"\"在n个变量上累加\"\"\"\n",
    "#     def __init__(self, n):\n",
    "#         self.data = [0.0] * n\n",
    "\n",
    "#     def add(self, *args):\n",
    "#         self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "#     def reset(self):\n",
    "#         self.data = [0.0] * len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.data[idx]\n",
    "    \n",
    "# # 用来跟踪训练过程中的指标（损失、准确度等）\n",
    "# class Accumulator:\n",
    "#     def __init__(self, n):\n",
    "#         self.data = [0.0] * n\n",
    "    \n",
    "#     def add(self, *args):\n",
    "#         for i, arg in enumerate(args):\n",
    "#             self.data[i] += arg\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.data[idx]\n",
    "    \n",
    "class Accumulator:\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n  # Initialize with 0.0 (floats)\n",
    "    \n",
    "    def add(self, *args):\n",
    "        # Ensure that the inputs are valid numeric values (float or int)\n",
    "        for i, arg in enumerate(args):\n",
    "            if isinstance(arg, (int, float)):\n",
    "                self.data[i] += arg  # Add the numeric value\n",
    "            else:\n",
    "                raise ValueError(f\"Expected numeric value, got {type(arg)}\")\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "\n",
    "# #@save\n",
    "# def train_epoch(net, train_iter, loss, updater):  \n",
    "#     metric = Accumulator(3)# 训练损失总和、训练准确度总和、样本数\n",
    "#     if isinstance(updater, gluon.Trainer):\n",
    "#         updater = updater.step\n",
    "#     for X, y in train_iter:\n",
    "#         # 计算梯度并更新参数\n",
    "#         with autograd.record():\n",
    "#             y_hat = net(X)\n",
    "#             l = loss(y_hat, y)\n",
    "#         l.backward()\n",
    "#         updater(X.shape[0])\n",
    "#         metric.add(float(l.sum()), accuracy(y_hat, y), y.shape[0])# 返回训练损失和训练精度\n",
    "#     return metric[0] / metric[2], metric[1] / metric[2]\n",
    "\n",
    "#@save\n",
    "class Animator:  \n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 2.5)):\n",
    "        # 增量地绘制多条线\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        use_svg_display()\n",
    "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        # 使用lambda函数捕获参数\n",
    "        self.config_axes = lambda:  set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # 向图表中添加多个数据点\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "# def accuracy(y_hat, y):  #@save\n",
    "#     \"\"\"计算预测正确的数量\"\"\"\n",
    "#     if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "#         y_hat = y_hat.argmax(axis=1)\n",
    "#     cmp = y_hat.astype(y.dtype) == y\n",
    "#     return float(cmp.astype(y.dtype).sum())\n",
    "\n",
    "def use_svg_display():  #@save\n",
    "    \"\"\"使用svg格式在Jupyter中显示绘图\"\"\"\n",
    "    backend_inline.set_matplotlib_formats('svg')\n",
    "    \n",
    "# def evaluate_accuracy(net, data_iter):  #@save\n",
    "#     \"\"\"计算在指定数据集上模型的精度\"\"\"\n",
    "#     metric = Accumulator(2)  # 正确预测数、预测总数\n",
    "#     for X, y in data_iter:\n",
    "#         metric.add(accuracy(net(X), y),  y.size)\n",
    "#     return metric[0] / metric[1]\n",
    "\n",
    "# 测试模型的准确度\n",
    "def evaluate_accuracy(net, data_iter, device):\n",
    "    if isinstance(net, nn.Module):\n",
    "        net.eval()  # 设置为评估模式\n",
    "        if not device:\n",
    "            device = next(iter(net.parameters())).device\n",
    "    metric = Accumulator(2)  # 正确预测数、预测总数\n",
    "#     net.eval()  # 设置为评估模式\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(X, list):\n",
    "                # BERT微调所需的（之后将介绍）\n",
    "                X = [x.to(device) for x in X]\n",
    "            else:\n",
    "                X = X.to(device)\n",
    "            y.to(device)\n",
    "            metric.add(accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]\n",
    "    \n",
    "def get_fashion_mnist_labels(labels):  #@save\n",
    "    \"\"\"返回Fashion-MNIST数据集的文本标签\"\"\"\n",
    "#     text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "#                    'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return [text_labels[int(i)] for i in labels]\n",
    "\n",
    "# def relu(X):\n",
    "#     return np.maximum(X, 0)\n",
    "# in_channels（输入通道数）\n",
    "# 表示输入数据的通道数。例如，对于 RGB 图像，in_channels=3（红、绿、蓝三个通道）；对于灰度图像，in_channels=1。\n",
    "# 类型：整数。\n",
    "# out_channels（输出通道数）\n",
    "# 表示卷积操作后输出的通道数，也就是卷积核的个数。每个卷积核生成一个输出特征图。\n",
    "# 类型：整数。\n",
    "# kernel_size（卷积核大小）\n",
    "# 定义卷积核的尺寸，可以是整数（如 3 表示 3x3）或元组（如 (3, 5) 表示 3x5）。\n",
    "# 常见的卷积核大小有 3x3、5x5 等。\n",
    "# stride（步幅，默认为 1）\n",
    "# 控制卷积核滑动的步长。步幅越大，输出尺寸越小。\n",
    "# 类型：整数或元组。\n",
    "# padding（填充，默认为 0）\n",
    "# 在输入边缘填充的像素数，用于控制输出尺寸。如果 padding=0，边缘信息可能丢失；适当填充可以保持输出尺寸。\n",
    "# 类型：整数或元组。\n",
    "# dilation（膨胀系数，默认为 1）\n",
    "# 控制卷积核中元素之间的间距，用于 dilated convolution（空洞卷积）。默认值为 1 表示普通卷积。\n",
    "# 类型：整数或元组。\n",
    "# groups（分组卷积，默认为 1）\n",
    "# 将输入和输出通道分成几组，分别进行卷积。默认值为 1 表示标准卷积；若设为 in_channels，则变为深度卷积（Depthwise Convolution）。\n",
    "# 类型：整数。\n",
    "# bias（偏置，默认为 True）\n",
    "# 是否为输出添加偏置项。通常保持默认即可。\n",
    "# 假设有一个简单的 CNN 模型\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 5 * 5, 4096), nn.ReLU(), nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# def net(X):\n",
    "#     X = X.reshape((-1, num_inputs))\n",
    "#     H = relu(np.dot(X, W1) + b1)\n",
    "#     return np.dot(H, W2) + b2\n",
    "\n",
    "\n",
    "# def updater(batch_size):\n",
    "#     return sgd(params, lr, batch_size)\n",
    "\n",
    "# def sgd(params, lr, batch_size):  # @save\n",
    "#     \"\"\"小批量随机梯度下降\"\"\"\n",
    "#     '''\n",
    "#     在机器学习中，.sgd 函数通常指的是随机梯度下降（Stochastic Gradient Descent）的缩写，它是一种用于优化模型参数的算法。\n",
    "#     随机梯度下降是梯度下降算法的一种变体，用于在大规模数据集上训练模型时减少计算成本。\n",
    "#     '''\n",
    "#     for param in params:\n",
    "#         param[:] = param - lr * param.grad / batch_size\n",
    "        \n",
    "# loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "\n",
    "def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):  #@save\n",
    "    \"\"\"绘制图像列表\"\"\"\n",
    "    figsize = (num_cols * scale, num_rows * scale)\n",
    "    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    for i, (ax, img) in enumerate(zip(axes, imgs)):\n",
    "        ax.imshow(img.numpy())\n",
    "        ax.axes.get_xaxis().set_visible(False)\n",
    "        ax.axes.get_yaxis().set_visible(False)\n",
    "        if titles:\n",
    "            ax.set_title(titles[i])\n",
    "    return axes\n",
    "\n",
    "# def train(net, train_iter, test_iter, loss, num_epochs, updater):  #@save\n",
    "#     \"\"\"训练模型（定义见第3章）\"\"\"\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     net = SimpleCNN().to(device)\n",
    "#     loss_fn = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "#     animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.2, 0.9],\n",
    "#                         legend=['train loss', 'train acc', 'test acc'])\n",
    "#     for epoch in range(num_epochs):\n",
    "#         train_metrics = train_epoch(net, train_iter, loss_fn, optimizer, device)\n",
    "#         test_acc = evaluate_accuracy(net, test_iter)\n",
    "#         animator.add(epoch + 1, train_metrics + (test_acc,))\n",
    "#     train_loss, train_acc = train_metrics\n",
    "# num_epochs, lr = 10, 0.1\n",
    "def train_epoch(net, train_iter, loss_fn, optimizer, device, animator):\n",
    "    metric = Accumulator(3)  # Track loss, accuracy, and sample count\n",
    "    net.train()\n",
    "    num_batches = len(train_iter)\n",
    "    for i, (X, y) in enumerate(train_iter):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        y_hat = net(X)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(y_hat, y)\n",
    "\n",
    "        # Backpropagation and optimizer step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "#             metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "        # Update metrics (make sure accuracy returns a numeric value)\n",
    "            metric.add(loss.item() * X.shape[0], accuracy(y_hat, y), X.shape[0])\n",
    "     \n",
    "    return metric[0] / metric[2], metric[1] / metric[2] \n",
    "\n",
    "\n",
    "def train(net, train_iter, test_iter, loss, num_epochs, lr, device):\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.0, 1.0],\n",
    "                        legend=['train loss', 'train acc', 'test acc'])\n",
    "    for epoch in range(num_epochs):\n",
    "#         train_metrics = train_epoch(net, train_iter, loss, optimizer, device, animator)\n",
    "        metric = Accumulator(3)  # Track loss, accuracy, and sample count\n",
    "        net.train()\n",
    "        num_batches = len(train_iter)\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            y_hat = net(X)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(y_hat, y)\n",
    "\n",
    "            # Backpropagation and optimizer step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "#             metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "        # Update metrics (make sure accuracy returns a numeric value)\n",
    "                metric.add(loss.item() * X.shape[0], accuracy(y_hat, y), X.shape[0])\n",
    "            train_l = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,(train_l, train_acc, None))\n",
    "        test_acc = evaluate_accuracy(net, test_iter, device)\n",
    "        animator.add(epoch + 1, (None, None, test_acc))\n",
    "#         animator.add(epoch + 1, metric[0] / metric[2], metric[1] / metric[2] + (test_acc,))\n",
    "    return metric[0] / metric[2], metric[1] / metric[2] \n",
    "\n",
    "# 数据加载\n",
    "batch_size = 256\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 模型、损失函数和优化器\n",
    "net = AlexNet().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "train_loss, train_acc = train(net, train_iter, test_iter, loss_fn, num_epochs=10, lr=0.01, device=device)\n",
    "\n",
    "def vgg_block(num_convs, in_channels, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels,\n",
    "                                kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def vgg(conv_arch):\n",
    "    conv_blks = []\n",
    "    in_channels = 1\n",
    "    # 卷积层部分\n",
    "    for (num_convs, out_channels) in conv_arch:\n",
    "        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))\n",
    "        in_channels = out_channels\n",
    "\n",
    "    return nn.Sequential(\n",
    "        *conv_blks, nn.Flatten(),\n",
    "        # 全连接层部分\n",
    "        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 10))\n",
    "\n",
    "# def predict(net, test_iter, n=100):  #@save\n",
    "#     for X, y in test_iter:\n",
    "#         break\n",
    "#     trues = get_fashion_mnist_labels(y)\n",
    "#     preds = get_fashion_mnist_labels(net(X).argmax(axis=1))\n",
    "#     titles = [true +'\\n' + pred for true, pred in zip(trues, preds)]\n",
    "#     show_images(X[:n].cpu().squeeze(1), 1, n, titles=titles[:n])\n",
    "    \n",
    "def predict(net, test_iter, device, n=10):  # Added device parameter, reduced n for clarity\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_iter:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            break\n",
    "        trues = get_fashion_mnist_labels(y)\n",
    "        preds = get_fashion_mnist_labels(net(X).cpu().argmax(dim=1))  # Fixed axis to dim\n",
    "        titles = [true + '\\n' + pred for true, pred in zip(trues, preds)]\n",
    "        show_images(X.cpu()[:n].squeeze(1), 1, n, titles=titles[:n])\n",
    "    \n",
    "predict(net, test_iter, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
